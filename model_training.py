# -*- coding: utf-8 -*-
"""Masters Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VZKq0ycw4ZZKVOc4T_HKzCD-XzYYog-L

# Imports
"""

!pip install -U --quiet accelerate transformers datasets evaluate

import os, re, shutil, warnings, torch, evaluate, gc

import numpy as np
import pandas as pd

from torch.utils.data import DataLoader
from sklearn.model_selection import train_test_split
from numba import cuda
from google.colab import drive
import os
from datetime import datetime

from transformers import (
    BartTokenizer, BartForConditionalGeneration,
    MT5Tokenizer, MT5TokenizerFast, MT5ForConditionalGeneration,
    Trainer, Seq2SeqTrainer, TrainingArguments, Seq2SeqTrainingArguments,
    DataCollatorForSeq2Seq, GenerationConfig, TrainerCallback, MarianTokenizer,
    MarianMTModel
)

drive.mount('/content/drive/')

warnings.filterwarnings("ignore", category=UserWarning, module="transformers.trainer")

# Commented out IPython magic to ensure Python compatibility.
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {DEVICE}")

ROOT = f"/content/drive/MyDrive/Університет/Diploma/Masters/"
# %cd {ROOT}

!echo
!echo Currently in:
# %pwd

"""# Classes"""

class PyTorchDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels['input_ids'][idx])
        return item

    def __len__(self):
        return len(self.labels['input_ids'])



class CustomTrainer(Seq2SeqTrainer):
    def __init__(self, *args, save_every_n_epochs=10, **kwargs):
        super().__init__(*args, **kwargs)
        self.save_every_n_epochs = save_every_n_epochs

    def save_model(self, output_dir=None, state_dict=None, save_format=None, push_to_hub=False, **kwargs):
        epoch = int(self.state.epoch)
        print(f"✅ Saving model at epoch {epoch} → {output_dir}")
        super().save_model(output_dir=output_dir, **kwargs)





class SamplePredictionCallback(TrainerCallback):
    def __init__(self, tokenizer, model, text_in_dataset, text_in_dataset_refference, text_not_in_dataset, max_length=128, device=None, log_file="training_log.txt"):
        self.tokenizer = tokenizer
        self.model = model
        self.text_in_dataset = text_in_dataset
        self.text_in_dataset_refference = text_in_dataset_refference
        self.text_not_in_dataset = text_not_in_dataset
        self.max_length = max_length
        self.device = device if device else "cpu"
        self.log_file = log_file

    def log(self, message):
        print(message)
        with open(self.log_file, "a", encoding="utf-8") as f:
            f.write(message + "\n")

    def predict(self, input_text):
        # Encode input
        inputs = self.tokenizer(input_text, return_tensors="pt").to(self.device)

        # Generate translation
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_length=self.max_length,
                num_beams=5,
                early_stopping=True,
                decoder_start_token_id=self.tokenizer.pad_token_id
            )
        # Decode output
        decoded_prediction = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        return decoded_prediction


    def on_epoch_end(self, args, state, control, model=None, trainer=None, **kwargs):
        # Move model to device if needed
        self.model.eval()
        self.model.to(self.device)

        translation_text_in_dataset = self.predict(self.text_in_dataset)
        translation_text_not_in_dataset = self.predict(self.text_not_in_dataset)

        log_message = (
            f"\n=== Epoch End Sample Prediction ===\n"
            f"Epoch: {state.epoch}\n"
            f"Text in dataset: {self.text_in_dataset}\n"
            f"Model Translation: {translation_text_in_dataset}\n"
            f"Text in dataset Refference: {self.text_in_dataset_refference}\n\n"
            f"Text NOT in dataset: {self.text_not_in_dataset}\n"
            f"Model Translation: {translation_text_not_in_dataset}\n"
            f"==================================\n"
        )
        self.log(log_message)

"""# Utility Functions"""

# Commented out IPython magic to ensure Python compatibility.
def split_into_sentences(text, f_=10000000):
    digits = "([0-9])"
    multiple_dots = r'\.{2,}'

    text = " " + text + "  "
    text = text.replace("\n"," ")
    text = re.sub(digits + "[.]" + digits,"\\1<prd>\\2",text)
    text = re.sub(multiple_dots, lambda match: "<prd>" * len(match.group(0)) + "<stop>", text)

    if "”" in text: text = text.replace(".”","”.")
    if "\"" in text: text = text.replace(".\"","\".")
    if "!" in text: text = text.replace("!\"","\"!")
    if "?" in text: text = text.replace("?\"","\"?")
    text = text.replace(".",".<stop>")
    text = text.replace("?","?<stop>")
    text = text.replace("!","!<stop>")
    # text = re.sub(r'([?!])(?!(?:["”„«»]))', r'\1<stop>', text)
    text = text.replace("<prd>",".")
    sentences = text.split("<stop>")
    sentences = [s[:f_].strip() for s in sentences]
    if sentences and not sentences[-1]: sentences = sentences[:-1]
    return sentences


def load_data(DEBUG=False, text_id=0, n=1000, file_names=[]):
#     %pwd
    OLD_UKRAINIAN_FILE_PATH = f'Training Data/Old Ukrainian/{style}'
    MODERN_UKRAINIAN_FILE_PATH = f'Training Data/Modern Ukrainian/{style}'
    old_ukrainian = file_names[0]
    modern_ukrainian = file_names[1]

    old_ukrainian_data = []
    modern_ukrainian_data = []
    if DEBUG:
        for src_path, tgt_path in zip([old_ukrainian[text_id]], [modern_ukrainian[text_id]]):
            with open(os.path.join(OLD_UKRAINIAN_FILE_PATH, src_path), "r", encoding='utf-8') as src_f, \
                    open(os.path.join(MODERN_UKRAINIAN_FILE_PATH, tgt_path), "r", encoding='utf-8') as tgt_f:
                original_splitted = split_into_sentences(src_f.read())
                modern_splitted = split_into_sentences(tgt_f.read())
                # =======Sentence Counter=========
                # new_original = []
                # new_modern = []
                # for i in range(len(original_splitted)):
                #     new_original.append((i, original_splitted[i]))
                # for i in range(len(modern_splitted)):
                #     new_modern.append((i, modern_splitted[i]))
                #
                # original_splitted = new_original
                # modern_splitted = new_modern
                # =======Sentence Counter=========
                old_ukrainian_data.extend(original_splitted)
                modern_ukrainian_data.extend(modern_splitted)
                # print(original_splitted)
                # print(modern_splitted)

        old_ukrainian_data = old_ukrainian_data[:n]
        modern_ukrainian_data = modern_ukrainian_data[:n]
    else:
        for src_path, tgt_path in zip(old_ukrainian, modern_ukrainian):
            with open(os.path.join(OLD_UKRAINIAN_FILE_PATH, src_path), "r", encoding='utf-8') as src_f, \
                    open(os.path.join(MODERN_UKRAINIAN_FILE_PATH, tgt_path), "r", encoding='utf-8') as tgt_f:
                original_splitted = split_into_sentences(src_f.read())
                modern_splitted = split_into_sentences(tgt_f.read())

                old_ukrainian_data.extend(original_splitted)
                modern_ukrainian_data.extend(modern_splitted)

    dataframe = pd.DataFrame({"source":modern_ukrainian_data, "target": old_ukrainian_data})
    return dataframe


from sklearn.model_selection import train_test_split

def preprocess_for_seq2seq(examples, tokenizer, model_type="t5",
                           max_input_length=512, max_target_length=128):
    """
    Preprocesses examples for seq2seq models (T5 or BART).

    Args:
        examples: DataFrame or object with 'source' and 'target' columns
        tokenizer: Hugging Face tokenizer
        model_type: "t5" or "bart"
        max_input_length: max tokens for input
        max_target_length: max tokens for output
        task_prefix: string to prepend to input text for T5

    Returns:
        train_dataset, val_dataset (PyTorch Datasets)
    """
    from sklearn.model_selection import train_test_split
    import torch

    # Split train/validation
    train_texts, val_texts, train_labels, val_labels = train_test_split(
        examples.source, examples.target, test_size=0.2, random_state=42
    )
    train_texts = train_texts.values.tolist()
    train_labels = train_labels.values.tolist()
    val_texts = val_texts.values.tolist()
    val_labels = val_labels.values.tolist()
    print("Train/val splits:", len(train_texts), len(train_labels), len(val_texts), len(val_labels))

    PADDING = "max_length"

    # Apply T5 task prefix if needed
    if model_type.lower() == "t5":
        task_prefix="Translate from modern Ukrainian into old Ukrainian style: "
        train_texts = [task_prefix + t for t in train_texts]
        val_texts = [task_prefix + t for t in val_texts]
    # Tokenize inputs
    train_encodings = tokenizer(
        train_texts,
        truncation=True,
        max_length=max_input_length,
        padding=PADDING,
        return_tensors="pt"
    )
    val_encodings = tokenizer(
        val_texts,
        truncation=True,
        max_length=max_input_length,
        padding=PADDING,
        return_tensors="pt"
    )

    # Tokenize targets
    train_labels_enc = tokenizer(
        text_target=train_labels,
        truncation=True,
        max_length=max_target_length,
        padding=PADDING,
        return_tensors="pt"
    )
    val_labels_enc = tokenizer(
        text_target=val_labels,
        truncation=True,
        max_length=max_target_length,
        padding=PADDING,
        return_tensors="pt"
    )

    # # Replace pad token IDs in labels with -100
    # label_pad_token_id = -100
    # train_labels_enc["input_ids"][train_labels_enc["input_ids"] == tokenizer.pad_token_id] = label_pad_token_id
    # val_labels_enc["input_ids"][val_labels_enc["input_ids"] == tokenizer.pad_token_id] = label_pad_token_id

    # Create PyTorch datasets
    train_dataset = PyTorchDataset(train_encodings, train_labels_enc)
    val_dataset = PyTorchDataset(val_encodings, val_labels_enc)

    return train_dataset, val_dataset



def postprocess_text(preds, labels):
    preds = [pred.strip() for pred in preds]
    labels = [[label.strip()] for label in labels]

    return preds, labels

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [label.strip() for label in decoded_labels]
    bleu_references = [[label] for label in decoded_labels]
    return bleu_metric.compute(predictions=decoded_preds, references=bleu_references)



def generate(text):
    tokens = tokenizer(text, truncation=True, padding=True, return_tensors="pt")["input_ids"]
    tokens = tokens.to(DEVICE)
    gen_tokens = model.generate(tokens)
    text = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)
    print(text)

"""# Configs"""

DEBUG = False
NUM_ROWS = 10

NUM_EPOCHS = 3 # Кількість епох навчання (повних проходів по всіх даних)

MAX_INPUT_LENGTH = 128  # Максимальна довжина вхідного тексту (у токенах)
MAX_TARGET_LENGTH = 512  # Максимальна довжина цільового тексту (у токенах)

TRAIN_BATCH_SIZE = 2  # Розмір batch під час навчання (кількість прикладів за один крок)
EVAL_BATCH_SIZE = 1   # Розмір batch під час оцінювання моделі

LEARNING_RATE = 3e-5  # Швидкість навчання (learning rate) — крок оновлення ваг моделі
WEIGHT_DECAY = 0.01   # Коефіцієнт регуляризації (зменшує перенавчання, штрафуючи великі ваги)
WARMUP_STEPS = 500    # Кількість кроків розігріву для поступового збільшення кроку навчання
FP16 = True            # Використання 16-бітної точності (прискорює навчання)

GRADIENT_ACCUMULATION_STEPS = 4  # Скільки кроків накопичувати градієнти перед оновленням ваг


SAVE_TOTAL_LIMIT = 2
NUM_BEAMS = 5

model_training = 'bart'
style = 'Кулішівка'
# style = 'Драгоманівка'

LOGS_PATH = os.path.join(ROOT, "Logs", style, model_training)
GOOGLE_COLAB_CHECKPOINT_PATH = os.path.join(ROOT, "Checkpoints", style, model_training)

# model_name = os.path.join(GOOGLE_COLAB_CHECKPOINT_PATH, "Epoch 2")

OLD_CHEKPOINT_FOLDER_PATH = os.path.join(GOOGLE_COLAB_CHECKPOINT_PATH, "Epoch 21")
display(os.path.exists(OLD_CHEKPOINT_FOLDER_PATH))
if not os.path.exists(OLD_CHEKPOINT_FOLDER_PATH):
   OLD_CHEKPOINT_FOLDER_PATH = None

model_name  = 'facebook/bart-base' if not OLD_CHEKPOINT_FOLDER_PATH else OLD_CHEKPOINT_FOLDER_PATH
display(model_name)
file_names = [
    ['Кайдашева_сімя̀ (-).txt', 'Вечери_на_хуторі_близь_Диканьки.txt'],
    [ 'Кайдашева_сімя̀_ChatGPT.txt','Вечери_на_хуторі_близь_Диканьки_ChatGPT.txt']

    # ['Хиба_ревуть_воли,_јак_јасла_повні_.txt'],
    # ['Хіба ревуть воли як ясла повні_ChatGPT.txt']
]
# Load the dataset
dataset = load_data(DEBUG=DEBUG, n=NUM_ROWS, file_names=file_names)
dataset

evaluation_config = {
    'Драгоманівка': {
        'sample_that_exists_in_text_old_ukrainian': 'Там стара мати заливајетьсьа гірькими сльозами, обнімајучи бриту голову синову; тут молода молодицьа, з дитинкоју на руках, голосить на весь мајдан, одніјеју рукоју схопивши за шију молодого чоловіка; а ось сестра з братом розмовльаје, сльозами доливајучи горе.',
        'sample_that_exists_in_text_modern_ukrainian': 'Там стара мати заливається гіркими сльозами, обнімаючи бриту голову синову; тут молода молодиця з дитиною на руках голосить на весь майдан, однією рукою схопивши за шию молодого чоловіка; а ось сестра з братом розмовляє, сльозами доливаючи горе.',
        'new_text_modern_ukrainian': 'Двоє братів стоять на схилі, тримаючи в руках палиці, і сміються, хоча сльози блищать на віях; поряд сестра, тримаючи візок з яблуками, тихо співає, а її пісня тягнеться над полем, торкаючись стежок, що ведуть у далечінь.'
    },
    'Кулішівка': {
        'sample_that_exists_in_text_old_ukrainian': 'Чорний кожухъ ёго бувъ розстяблений, шапку державъ вінъ у руці, пітъ зъ нёго котився градомъ.',
        'sample_that_exists_in_text_modern_ukrainian': 'Чорний кожух у нього був розстебнутий, шапку він тримав у руці, піт котився з нього градом.',
        'new_text_modern_ukrainian': 'Двоє братів стоять на схилі, тримаючи в руках палиці, і сміються, хоча сльози блищать на віях; поряд сестра, тримаючи візок з яблуками, тихо співає, а її пісня тягнеться над полем, торкаючись стежок, що ведуть у далечінь.'
    },
}

"""# Bart:

"""

if model_training=='bart':
    tokenizer = BartTokenizer.from_pretrained(model_name)
    model = BartForConditionalGeneration.from_pretrained(model_name)
    # model = model.to(DEVICE)
    bleu_metric = evaluate.load('bleu')
    data_collator = DataCollatorForSeq2Seq(
        tokenizer,
        model=model,
        label_pad_token_id=-100,
        padding='max_length',
        max_length=MAX_INPUT_LENGTH
    )

    train_dataset, val_dataset = preprocess_for_seq2seq(
        dataset,
        tokenizer,
        max_target_length=MAX_TARGET_LENGTH,
        max_input_length=MAX_INPUT_LENGTH,
        model_type='bart'
    )
    generation_parameters = {
        "max_length": MAX_TARGET_LENGTH,           # adjust depending on your sentence lengths
        "num_beams": NUM_BEAMS,              # 4–6 beams is usually sufficient
        "early_stopping": True,      # stops once EOS token is generated
        "decoder_start_token_id": tokenizer.pad_token_id
    }
    generation_config = GenerationConfig(**generation_parameters)

    training_args = Seq2SeqTrainingArguments(
        output_dir=GOOGLE_COLAB_CHECKPOINT_PATH,
        eval_strategy="epoch",
        save_strategy="epoch",
        logging_strategy="epoch",
        per_device_train_batch_size=TRAIN_BATCH_SIZE,
        per_device_eval_batch_size=EVAL_BATCH_SIZE,
        num_train_epochs=NUM_EPOCHS,
        learning_rate=LEARNING_RATE,
        weight_decay=WEIGHT_DECAY,
        warmup_steps=WARMUP_STEPS,
        generation_max_length=MAX_TARGET_LENGTH,
        generation_num_beams=NUM_BEAMS,
        fp16=FP16,
        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
        save_total_limit=SAVE_TOTAL_LIMIT,
        logging_dir="./logs",
        dataloader_num_workers=2,
        predict_with_generate=True,
        generation_config=generation_config,
        report_to='tensorboard'

    )


    style_eval_config = evaluation_config[style]

    trainer = CustomTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
        callbacks=[
            SamplePredictionCallback(tokenizer,
              model,
              style_eval_config['sample_that_exists_in_text_modern_ukrainian'],
              style_eval_config['new_text_modern_ukrainian'],
              style_eval_config['new_text_modern_ukrainian'],
              max_length=MAX_TARGET_LENGTH,
              device=("cuda" if torch.cuda.is_available() else None),
              log_file=LOGS_PATH,
              )
            ]
        )

    trainer.train()
    print(f"Log saved to: {LOGS_PATH}")

from google.colab import drive
drive.mount('/content/drive')

GOOGLE_COLAB_CHECKPOINT_PATH

!zip -r /content/drive/MyDrive/Університет/Diploma/Masters/Checkpoints/{style}/{model_training}/Epoch 9.zip "/content/drive/MyDrive/Університет/Diploma/Masters/Checkpoints/{style}/{model_training}/checkpoint-2445"
    # from google.colab import files
    # files.download(f"/content/drive/MyDrive/Університет/Diploma/Masters/Checkpoints/{style}/{model_training}/Epoch {int(trainer.state.epoch)}.zip")



"""# T5:"""

if model_training=='t5':
    model_name = "google/mt5-small"  # or "t5-small" for low-memory, "t5-large" for high-quality
    tokenizer = MT5Tokenizer.from_pretrained(model_name)
    model = MT5ForConditionalGeneration.from_pretrained(model_name)
    # model = model.to(DEVICE)
    bleu_metric = evaluate.load('bleu')
    data_collator = DataCollatorForSeq2Seq(
            tokenizer,
            model=model,
            label_pad_token_id=-100,
            padding='max_length',
            max_length=MAX_INPUT_LENGTH
        )


    model.config.pad_token_id = tokenizer.pad_token_id
    model.config.eos_token_id = tokenizer.eos_token_id
    model.config.decoder_start_token_id = tokenizer.pad_token_id

    train_dataset, val_dataset = preprocess_for_seq2seq(dataset, tokenizer, max_target_length=MAX_TARGET_LENGTH, max_input_length=MAX_INPUT_LENGTH, model_type='t5')
    generation_parameters = {
        "max_length": 512,
        # "temperature": 0.4,
        # "top_k": 50,
        # "top_p": 0.95,
        # "num_beams": 2,
        # "do_sample": True,
        "decoder_start_token_id": model.config.decoder_start_token_id
    }

    generation_config = GenerationConfig(**generation_parameters)

    training_args = Seq2SeqTrainingArguments(
        output_dir="./t5_results",
        eval_strategy="epoch",
        save_strategy="epoch",
        logging_strategy="epoch",
        per_device_train_batch_size=TRAIN_BATCH_SIZE,
        per_device_eval_batch_size=EVAL_BATCH_SIZE,
        num_train_epochs=NUM_EPOCHS,
        learning_rate=LEARNING_RATE,
        weight_decay=WEIGHT_DECAY,
        warmup_steps=WARMUP_STEPS,
        # fp16=FP16,
        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
        save_total_limit=SAVE_TOTAL_LIMIT,
        logging_dir="./logs",
        dataloader_num_workers=2,
        load_best_model_at_end=True,
        predict_with_generate=True,
        generation_config=generation_config,
        report_to='tensorboard'

    )

    trainer = CustomTrainer(
        model=model,
        args=training_args,
        compute_metrics=compute_metrics,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator

    )

    # # Save the model
    trainer.train()

"""# MarianMT"""

if model_training=='MarianMT':
    model_name = "Helsinki-NLP/opus-mt-en-uk"
    tokenizer = MarianTokenizer.from_pretrained(model_name)
    model = MarianMTModel.from_pretrained(model_name)

    model.config.pad_token_id = tokenizer.pad_token_id
    model.config.eos_token_id = tokenizer.eos_token_id
    model.config.decoder_start_token_id = tokenizer.pad_token_id  # required by MarianMT

    bleu_metric = evaluate.load('bleu')

    data_collator = DataCollatorForSeq2Seq(
        tokenizer,
        model=model,
        label_pad_token_id=-100,
        padding='max_length',
        max_length=MAX_INPUT_LENGTH
    )

    train_dataset, val_dataset = preprocess_for_seq2seq(
        dataset,
        tokenizer,
        max_target_length=MAX_TARGET_LENGTH,
        max_input_length=MAX_INPUT_LENGTH,
        model_type='marian'
    )


    generation_parameters = {
        "max_length": 128,           # adjust depending on your sentence lengths
        "num_beams": 5,              # 4–6 beams is usually sufficient
        "early_stopping": True,      # stops once EOS token is generated
        "decoder_start_token_id": tokenizer.pad_token_id
    }

    generation_config = GenerationConfig(**generation_parameters)
    training_args = Seq2SeqTrainingArguments(
        output_dir=GOOGLE_COLAB_CHECKPOINT_PATH,
        eval_strategy="epoch",
        save_strategy="epoch",
        logging_strategy="epoch",
        per_device_train_batch_size=TRAIN_BATCH_SIZE,
        per_device_eval_batch_size=EVAL_BATCH_SIZE,
        num_train_epochs=NUM_EPOCHS,
        learning_rate=LEARNING_RATE,
        weight_decay=WEIGHT_DECAY,
        warmup_steps=WARMUP_STEPS,
        # fp16=FP16,
        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
        save_total_limit=SAVE_TOTAL_LIMIT,
        logging_dir="./logs",
        dataloader_num_workers=2,
        load_best_model_at_end=True,
        predict_with_generate=True,
        generation_config=generation_config,
        report_to='tensorboard'

    )

    style_eval_config = evaluation_config[style]

    trainer = CustomTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
        callbacks=[
            SamplePredictionCallback(tokenizer,
              model,
              style_eval_config['sample_that_exists_in_text_modern_ukrainian'],
              style_eval_config['new_text_modern_ukrainian'],
              style_eval_config['new_text_modern_ukrainian'],
              max_length=MAX_TARGET_LENGTH,
              device=("cuda" if torch.cuda.is_available() else None),
              log_file=os.path.join(LOGS_PATH, style, 'logs.txt'))
            ],
        save_every_n_epochs=1)

    trainer.train()
    # %mv {os.path.join(LOGS_PATH, 'logs.txt')} {os.path.join(GOOGLE_COLAB_CHECKPOINT_PATH, 'logs.txt')}
    # shutil.move(os.path.join(LOGS_PATH, 'logs.txt'), LOGS_PATH)
    print(f"Log saved to: {LOGS_PATH}")